Реализовано 5 примеров из книги Шолле Франсуа  Глубокое обучение на Python. 2-е межд. издание.

1 ГЕНЕРИРОВАНИЕ ТЕКСТА
Суть главы заключается в демонстрации того, как обучить RNN моделировать вероятностное распределение текста, 
а затем использовать эту модель для генерации нового текста, похожего на обучающий корпус.
Реализация:
https://colab.research.google.com/drive/1wdpNPMAaDV2_huY6zS_aKOmmRrx1EpS-#scrollTo=GRKvVPM41UWQ

2 DEEPDREAM
DeepDream — техника визуализации, которая усиливает признаки, обнаруженные в нейронной сети, 
путем градиентного подъема на основе предварительно обученной сверточной сети.

  1. Используем предварительно обученную сеть для извлечения признаков из разных слоев
  2. Взвешиваем вклад каждого слоя, чтобы управлять тем, какие признаки будут усилены
  3. Используем градиентный подъем, чтобы изменить входное изображение так, чтобы активации выбранных слоев 
  (потери) становились больше, тем самым усиливая признаки, которые сеть считает важными

3 НЕЙРОННАЯ ПЕРЕДАЧА СТИЛЯ
Нейронная передача стиля заключается в применении стиля изображения-образца к целевому изображению 
при сохранении содержимого этого целевого изображения  
  1. Загружаем изображения
  2. Используем вспомогательные функции
  3. Используем предварительно обученную сверточную сеть и создадим модель,
     извлекающую признаки и возвращающую активации промежуточных слоев
  4. Определяемм функцию потерь содержимого, которая позволит гарантировать сходство представлений целевого (исходного)
     и сгенерированного изображений в верхнем слое сети
  5. Определяемм функцию потерь стиля:
     Вычисляет матрицу Грама из входной карты признаков x. Матрица Грама описывает корреляцию между разными каналами в карте признаков
     Вычисляет скалярное значение, которое показывает, насколько сильно различается стиль у двух изображений
  6. Определяемм функцию общей потери вариации. Вычисляет разницу между соседними пикселями сгенерированного изображения
     и стремится её минимизировать
  7. Определяемм функцию общей потери вариации, которая  будет минимизироваться
  8. Настройка процесса градиентного спуска
     
4 ГЕНЕРИРОВАНИЕ ИЗОБРАЖЕНИЙ С ВАРИАЦИОННЫМИ АВТОКОДИРОВЩИКАМИ
Загружает данные MNIST, создает экземпляр модели VAE, обучает ее и затем, чтобы оценить как модель работает, 
генерирует изображения из разных точек в скрытом пространстве. Для каждой точки из сетки, которая покрывает скрытое пространство, 
мы сэмплируем точку и передаем ее через декодировщик, получая новое сгенерированное изображение. 
Затем мы визуализируем все эти сгенерированные изображения в виде сетки и сохраняем ее на диск. 
Таким образом, мы можем видеть, как изображения меняются при движении по скрытому пространству, что позволяет 
оценить эффективность обучения VAE

5 ГЕНЕРАТИВНО-СОСТЯЗАТЕЛЬНАЯ СЕТЬ
Общая схема GAN:
  •	Генератор: Эта нейросеть преобразует векторы из скрытого пространства (определенной размерности) в изображения размером 64x64 
  с тремя цветовыми каналами.
  •	Дискриминатор: Эта нейросеть принимает на вход изображения размером 64x64 с тремя цветовыми каналами и выдает 
  оценку вероятности того, что это изображение является реальным.
  •	Составная сеть GAN: Она объединяет генератор и дискриминатор: gan(x) = discriminator(generator(x)). 
  То есть, она преобразует векторы из скрытого пространства в оценку их реалистичности после декодирования генератором.
Процесс обучения:
  1. Обучение дискриминатора: Дискриминатор обучается на примерах реальных и поддельных изображений с соответствующими метками
     (реальное/поддельное). Это обучение ничем не отличается от обучения обычной модели классификации изображений.
  2. Обучение генератора: Генератор обучается на основе градиентов своих весов по отношению к потерям составной сети gan.
     На каждом шаге веса генератора смещаются в направлении, которое увеличивает вероятность того, что дискриминатор
     классифицирует сгенерированные изображения как реальные. Другими словами, мы учим генератор обманывать дискриминатор.
